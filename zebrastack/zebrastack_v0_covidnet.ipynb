{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the Latent Space of CXR Patient Geometry\n",
    "## Zebrastack V0 VAE trained on the CovidNet chest radiograph dataset\n",
    "The zebrastack is a variational auto-encoder that is _very_ roughly aligned with the distributed hierarchical architecture + shifter circuit.  Here it is applied to recognition of the CovidNet chest radiograph dataset.\n",
    "\n",
    "First we load some center-surround processed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_processing: DATA_ALL=G:\\DataAll; DATA_TEMP=E:\\Data\\zebtrastack_temp\n",
      "Loaded 30001 npy in 87.7020845413208 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common.image_preprocessing import data_all, data_temp, temp_from_original\n",
    "from pathlib import Path, PurePath\n",
    "import time\n",
    "\n",
    "sz = 128\n",
    "temp_relative_path = PurePath(f\"{sz}x{sz}\") / 'channelized_clahe_processed'\n",
    "cxr8_original_path = data_all / 'NIH_Cxr8' / 'by_class' / 'no_finding'\n",
    "cxr8_temp = temp_from_original(cxr8_original_path, temp_relative_path)\n",
    "processed_imgs = {}\n",
    "\n",
    "start_time = time.time()\n",
    "for npy_filepath in cxr8_temp.glob('*.npy'):\n",
    "    img = np.load(npy_filepath)\n",
    "    # print(img.shape)\n",
    "    # img = np.reshape(img, (img.shape[],img.shape[1],4))\n",
    "    processed_imgs[npy_filepath.stem] = img\n",
    "    if len(processed_imgs) % 100 == 0:\n",
    "        print(f\"{npy_filepath.stem}: {img.shape}    \", end='\\r')\n",
    "    if len(processed_imgs) > 30000:\n",
    "        break\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Loaded {len(processed_imgs)} npy in {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the data in to train and test: 90% train and 10% test.  We are assuming all images are from distinct patients, so there is no need to partition by subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has shape (27001, 1, 128, 128, 4)\n",
      "Testing data has shape (3000, 1, 128, 128, 4)\n"
     ]
    }
   ],
   "source": [
    "test_count = int(len(processed_imgs)/10)\n",
    "processed_img_list = list(processed_imgs.values())\n",
    "x_test = processed_img_list[:test_count]\n",
    "x_train = processed_img_list[test_count:]\n",
    "x_train, x_test = np.array(x_train), np.array(x_test)\n",
    "print(f\"Training data has shape {x_train.shape}\")\n",
    "print(f\"Testing data has shape {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], 128, 128, 4))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 128, 128, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3186288  -0.2491688  -0.02682215  0.        ] [0.46276423 0.40109926 0.4401671  0.54857194] [0.78139305 0.6502681  0.46698925 0.54857194]\n"
     ]
    }
   ],
   "source": [
    "min_all = np.min(x_train,axis=(0,1,2))\n",
    "max_all = np.max(x_train,axis=(0,1,2))\n",
    "width_all = max_all - min_all\n",
    "print(min_all, max_all, width_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train - min_all) / width_all\n",
    "x_test = (x_test - min_all) / width_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_min_all = np.min(x_train,axis=(0,1,2))\n",
    "re_max_all = np.max(x_train,axis=(0,1,2))\n",
    "re_width_all = re_max_all - re_min_all\n",
    "print(re_min_all, re_max_all, re_width_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Variational Autoencoder\n",
    "To defined the variational autoencoder, we need to helper functions:\n",
    "* A function for reparameterized sampling\n",
    "* A function for KLDiv loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cxr_projection.zebrastack_v0_model import create_autoencoder, create_encoder, create_decoder\n",
    "retina, encoder, shape, [z_mean, z_log_var, z] = create_encoder()\n",
    "decoder = create_decoder(shape)\n",
    "autoencoder = create_autoencoder(retina, encoder, [z_mean, z_log_var, z], decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training the model\n",
    "Train by calling .fit with the training data.  \n",
    "* Batch size of 512 helps convergence, but causes resource exhaustion > 128x128.\n",
    "* Epochs > 1024 take a while, but tend to lead to better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train, \n",
    "                epochs=512, batch_size=256, \n",
    "                shuffle=True, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Reconstructing Images\n",
    "To qualitatively assess the model, look at a few images and their reconstructions from latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "\n",
    "# @lru_cache(maxsize=None)\n",
    "def decode_latent(decoder, latent_vector:tuple):\n",
    "    \"\"\"\n",
    "    helper to decode latent, with caching to speed up\n",
    "    # Arguments\n",
    "        decode_only: model for decoding\n",
    "        latent_vector: _tuple_ representing the vector to be decoded\n",
    "    # Returns\n",
    "        decoded image\n",
    "    \"\"\"\n",
    "    latent_vector_arr = np.array([latent_vector])\n",
    "    return decoder.predict(latent_vector_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cxr_projection.show_original_decoded import show_grayscale, show_original_decoded\n",
    "\n",
    "encode_only_imgs = encoder.predict(x_test[:10])\n",
    "decoded_imgs = \\\n",
    "    [decode_latent(decoder, tuple(latent_vector)) \n",
    "         for latent_vector in encode_only_imgs[2]]\n",
    "# show_original_decoded(x_test[:10], decoded_imgs, sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(2,2)\n",
    "for n in range(4):\n",
    "    level_img = np.reshape(decoded_imgs[6][...,n], (128,128))\n",
    "    axs[n // 2][n % 2].imshow(level_img, cmap='plasma') # , vmin=-.5, vmax=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.filter_banks import make_gauss_kernels, make_dog_kernels, show_filter_bank\n",
    "dog_kernels_tf = make_dog_kernels()\n",
    "show_filter_bank(dog_kernels_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_all = [-0.3100704,  -0.23559849, -0.02646105,  0.        ] \n",
    "# max_all = [0.42579,    0.38077682, 0.44011822, 0.54857194]\n",
    "#width_all = [0.7358604,  0.6163753,  0.46657926, 0.54857194]\n",
    "\n",
    "re_decoded_img = decoded_imgs[6]\n",
    "re_decoded_img = width_all * re_decoded_img + min_all\n",
    "# print(decoded_imgs[6])\n",
    "print(re_decoded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import tensorflow as tf\n",
    "def mse_tf(current_tf:tf.Tensor, kernel_tf:tf.Tensor, match_tf:tf.Tensor):\n",
    "    current_out_tf = \\\n",
    "        tf.nn.conv2d(current_tf, kernel_tf,\n",
    "                     strides=[1, 1, 1, 1],\n",
    "                     padding='SAME')\n",
    "    loss = tf.reduce_mean((current_out_tf - match_tf) ** 2)\n",
    "    return loss\n",
    "\n",
    "sz = 128\n",
    "current_value = tf.Variable(np.zeros((1,sz,sz,1)), dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    current_out = mse_tf(current_value, dog_kernels_tf, re_decoded_img)\n",
    "grad = tape.gradient(current_out, current_value)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(5,3))\n",
    "axs[0].imshow(tf.reshape(current_value, (sz,sz)))\n",
    "axs[1].imshow(tf.reshape(grad, (sz,sz)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig, axs = plt.subplots(1,2,figsize=(5,3))\n",
    "learning_rate = 1e+3\n",
    "for n in range(100):\n",
    "    current_value.assign_sub(learning_rate * grad)\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_out = mse_tf(current_value, dog_kernels_tf, re_decoded_img)\n",
    "        print(f\"loss = {current_out}   \", end='\\r')\n",
    "    grad = tape.gradient(current_out, current_value)\n",
    "axs[0].imshow(tf.reshape(current_value, (sz,sz)), cmap='gray')\n",
    "axs[1].imshow(tf.reshape(grad, (sz,sz)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK? so then save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from common.image_preprocessing import data_all, data_temp\n",
    "\n",
    "weights_cxr8_temp = temp_from_original(cxr8_original_path, PurePath(f\"{sz}x{sz}\") / 'weights')\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "print(\"Saving \", weights_cxr8_temp / f\"{timestamp}_rgc_encoder.h5\")\n",
    "encoder.save_weights(weights_cxr8_temp / f\"{timestamp}_rgc_encoder.h5\")\n",
    "print(\"Saving \", weights_cxr8_temp / f\"{timestamp}_rgc_decoder.h5\")\n",
    "decoder.save_weights(weights_cxr8_temp / f\"{timestamp}_rgc_decoder.h5\")\n",
    "print(\"Saving \", weights_cxr8_temp / f\"{timestamp}_rgc_autoencoder.h5\")\n",
    "autoencoder.save_weights(weights_cxr8_temp / f\"{timestamp}_rgc_autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_processed_path = \"E:\\\\Data\\\\anat0mixer_temp\\\\shifter_processed\"\n",
    "for n in range(len(x_train_shifted)):\n",
    "    np.save(shift_processed_path+f\"\\\\x_train_shifted_{n}\", \n",
    "            x_train_shifted[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(x_test_shifted)):    \n",
    "    np.save(shift_processed_path+f\"\\\\x_test_shifted_{n}\", \n",
    "            x_test_shifted[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "x_train_fns = \\\n",
    "    Path('E:\\\\Data\\\\anat0mixer_temp\\\\shifter_processed') \\\n",
    "        .glob('x_train_shifted_*.npy')\n",
    "x_train_shifted = []\n",
    "for fn in x_train_fns:\n",
    "    x_train_shifted.append(np.load(fn))\n",
    "\n",
    "x_test_fns = \\\n",
    "    Path('E:\\\\Data\\\\anat0mixer_temp\\\\shifter_processed') \\\n",
    "        .glob('x_test_shifted_*.npy')\n",
    "x_test_shifted = []\n",
    "for fn in x_test_fns:\n",
    "    x_test_shifted.append(np.load(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_shifted = np.array(x_train_shifted)\n",
    "x_test_shifted = np.array(x_test_shifted)\n",
    "x_train_shifted.shape, x_test_shifted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train_shifted, x_train_shifted, \n",
    "                epochs=256, batch_size=128, \n",
    "                shuffle=True, validation_data=(x_test_shifted, x_test_shifted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "autoencoder.save_weights('E:\\\\Data\\\\anat0mixer_temp\\\\autoencoder_v1')\n",
    "encoder.save_weights('E:\\\\Data\\\\anat0mixer_temp\\\\encoder_v1')\n",
    "decoder.save_weights('E:\\\\Data\\\\anat0mixer_temp\\\\decoder_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_shift = encoder.predict(x_test_shifted[80:90])\n",
    "decoded_imgs_shift = \\\n",
    "    [decode_latent(decoder, tuple(latent_vector)) \n",
    "         for latent_vector in encoded_shift[2]]\n",
    "show_original_decoded(x_test_shifted[80:90], decoded_imgs_shift, sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "If the model output looks good, then save it for subsequent use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('data\\zebrastack_v0_covidnet_encoder_model')\n",
    "decoder.save('data\\zebrastack_v0_covidnet_decoder_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anat0Mixer Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image_for_sliders(**kwargs):\n",
    "    from scipy.ndimage import zoom\n",
    "    # print(tuple(kwargs.values()))\n",
    "    decoded_pixel_array = decode_latent(decoder, tuple(kwargs.values()))\n",
    "    interp_array = decoded_pixel_array.reshape(sz,sz)\n",
    "    # interp_array = zoom(decoded_pixel_array.reshape(sz,sz), 4.0, order=0)\n",
    "    # interp_array = interp_array.reshape(sz*4, sz*4)\n",
    "    # ax.imshow(interp_array, cmap='gray')\n",
    "    # a=im.get_array()    \n",
    "    im.set_array(interp_array)\n",
    "    fig.canvas.draw()\n",
    "    return kwargs['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from ipywidgets import FloatSlider, interactive_output, Layout, HBox, VBox\n",
    "\n",
    "kwargs = {}\n",
    "\n",
    "for k in range(latent_dim):\n",
    "    kwargs[str(k)] = \\\n",
    "        FloatSlider(value = 0,\n",
    "                    min = -2.0, max = 2.0, \n",
    "                    step = 0.01,\n",
    "                    orientation='vertical',\n",
    "                    layout=Layout(padding='0%'))\n",
    "\n",
    "decoded_pixel_array = decode_latent(decoder, tuple([slider.value for slider in kwargs.values()]))\n",
    "interp_array = decoded_pixel_array.reshape(sz,sz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,3))\n",
    "ax = fig.add_subplot()\n",
    "im = ax.imshow(interp_array, cmap='gray')\n",
    "\n",
    "slider_widgets = interactive_output(update_image_for_sliders, kwargs)\n",
    "slider_widgets.layout.height = '350px'\n",
    "\n",
    "import gc\n",
    "gc.disable()\n",
    "display(HBox(list(kwargs.values()), layout = Layout(padding='0%')))\n",
    "display(slider_widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit0c83dea770f14d06900285e7e16051d3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
