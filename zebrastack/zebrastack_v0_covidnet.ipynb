{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the Latent Space of CXR Patient Geometry\n",
    "## Zebrastack V0 VAE trained on the CovidNet chest radiograph dataset\n",
    "The zebrastack is a variational auto-encoder that is _very_ roughly aligned with the distributed hierarchical architecture + shifter circuit.  Here it is applied to recognition of the CovidNet chest radiograph dataset.\n",
    "\n",
    "First we load some center-surround processed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_processing: DATA_ALL=G:\\DataAll; DATA_TEMP=E:\\Data\\zebtrastack_temp\n",
      "Loaded 20000 npy in 39.67962718009949 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common.image_preprocessing import data_all, data_temp, temp_from_original\n",
    "from pathlib import Path, PurePath\n",
    "import time, os\n",
    "\n",
    "sz = 128\n",
    "# temp_relative_path = PurePath(f\"{sz}x{sz}\") / 'chest-nihcc' / '128x128' / 'clahe_processed'\n",
    "cxr8_original_path = data_all / 'NIH_Cxr8' / 'by_class' / 'no_finding'\n",
    "cxr8_temp = Path(os.environ['DATA_TEMP']) / 'chest-nihcc' / '128x128' / 'clahe_processed' # temp_from_original(cxr8_original_path, temp_relative_path)\n",
    "processed_imgs = {}\n",
    "\n",
    "start_time = time.time()\n",
    "for npy_filepath in list(cxr8_temp.glob('*.npy'))[:20000]:\n",
    "    img = np.load(npy_filepath)\n",
    "    # print(img.shape)\n",
    "    # img = np.reshape(img, (img.shape[],img.shape[1],4))\n",
    "    processed_imgs[npy_filepath.stem] = img\n",
    "    if len(processed_imgs) % 100 == 0:\n",
    "        print(f\"{npy_filepath.stem}: {img.shape}    \", end='\\r')\n",
    "    if len(processed_imgs) > 30000:\n",
    "        break\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Loaded {len(processed_imgs)} npy in {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the data in to train and test: 90% train and 10% test.  We are assuming all images are from distinct patients, so there is no need to partition by subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has shape (18000, 128, 128)\n",
      "Testing data has shape (2000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "test_count = int(len(processed_imgs)/10)\n",
    "processed_img_list = list(processed_imgs.values())\n",
    "x_test = processed_img_list[:test_count]\n",
    "x_train = processed_img_list[test_count:]\n",
    "x_train, x_test = np.array(x_train), np.array(x_test)\n",
    "print(f\"Training data has shape {x_train.shape}\")\n",
    "print(f\"Testing data has shape {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], 128, 128, 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 128, 128, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_all = np.min(x_train,axis=(0,1,2))\n",
    "# max_all = np.max(x_train,axis=(0,1,2))\n",
    "# width_all = max_all - min_all\n",
    "# print(min_all, max_all, width_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = (x_train - min_all) / width_all\n",
    "# x_test = (x_test - min_all) / width_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re_min_all = np.min(x_train,axis=(0,1,2))\n",
    "# re_max_all = np.max(x_train,axis=(0,1,2))\n",
    "# re_width_all = re_max_all - re_min_all\n",
    "# print(re_min_all, re_max_all, re_width_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Variational Autoencoder\n",
    "To defined the variational autoencoder, we need to helper functions:\n",
    "* A function for reparameterized sampling\n",
    "* A function for KLDiv loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cxr_projection.zebrastack_v0_model import create_autoencoder, create_encoder, create_decoder\n",
    "import tensorflow as tf\n",
    "tf.python.framework.ops.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"v1_to_pulvinar_encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "retina_128 (InputLayer)         [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "v1_conv2d (Conv2D)              (None, 128, 128, 32) 832         retina_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "v1_maxpool (MaxPooling2D)       (None, 64, 64, 32)   0           v1_conv2d[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "v1_dropout (SpatialDropout2D)   (None, 64, 64, 32)   0           v1_maxpool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "v2_conv2d (Conv2D)              (None, 64, 64, 64)   2112        v1_dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "v2_maxpool (MaxPooling2D)       (None, 32, 32, 64)   0           v2_conv2d[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "v4_conv2d (Conv2D)              (None, 32, 32, 64)   36928       v2_maxpool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "v4_maxpool (MaxPooling2D)       (None, 16, 16, 64)   0           v4_conv2d[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pit_conv2d (Conv2D)             (None, 16, 16, 64)   4160        v4_maxpool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pit_maxpool (MaxPooling2D)      (None, 8, 8, 64)     0           pit_conv2d[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "cit_conv2d (Conv2D)             (None, 8, 8, 64)     36928       pit_maxpool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ait_local (LocallyConnected2D)  (None, 6, 6, 2)      41544       cit_conv2d[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ait_regular (ActivityRegulariza (None, 6, 6, 2)      0           ait_local[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pulvinar_flatten (Flatten)      (None, 72)           0           ait_regular[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pulvinar_dense (Dense)          (None, 12)           876         pulvinar_flatten[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 12)           156         pulvinar_dense[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 12)           156         pulvinar_dense[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 12)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 123,692\n",
      "Trainable params: 123,692\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"pulvinar_to_v1_decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      [(None, 12)]              0         \n",
      "_________________________________________________________________\n",
      "pulvinar_dense_back (Dense)  (None, 72)                936       \n",
      "_________________________________________________________________\n",
      "pulvinar_antiflatten (Reshap (None, 6, 6, 2)           0         \n",
      "_________________________________________________________________\n",
      "ait_padding_back (ZeroPaddin (None, 8, 8, 2)           0         \n",
      "_________________________________________________________________\n",
      "ait_local_back (LocallyConne (None, 6, 6, 2)           1368      \n",
      "_________________________________________________________________\n",
      "cit_padding_back (ZeroPaddin (None, 8, 8, 2)           0         \n",
      "_________________________________________________________________\n",
      "cit_conv2d_trans (Conv2DTran (None, 8, 8, 64)          1216      \n",
      "_________________________________________________________________\n",
      "cit_upsample_back (UpSamplin (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "pit_conv2d_trans (Conv2DTran (None, 16, 16, 64)        4160      \n",
      "_________________________________________________________________\n",
      "pit_upsample_back (UpSamplin (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "v4_conv2d_trans (Conv2DTrans (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "v4_upsample_back (UpSampling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "v2_conv2d_trans (Conv2DTrans (None, 64, 64, 32)        2080      \n",
      "_________________________________________________________________\n",
      "v2_upsample_back (UpSampling (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "v1_conv2d_5x5_back (Conv2DTr (None, 128, 128, 1)       801       \n",
      "=================================================================\n",
      "Total params: 47,489\n",
      "Trainable params: 47,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"v1_to_pulvinar_vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "retina_128 (InputLayer)      [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "v1_to_pulvinar_encoder (Func [(None, 12), (None, 12),  123692    \n",
      "_________________________________________________________________\n",
      "pulvinar_to_v1_decoder (Func (None, 128, 128, 1)       47489     \n",
      "=================================================================\n",
      "Total params: 171,181\n",
      "Trainable params: 171,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "retina, encoder, shape, [z_mean, z_log_var, z] = create_encoder()\n",
    "decoder = create_decoder(shape)\n",
    "autoencoder = create_autoencoder(retina, encoder, [z_mean, z_log_var, z], decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training the model\n",
    "Train by calling .fit with the training data.  \n",
    "* Batch size of 512 helps convergence, but causes resource exhaustion > 128x128.\n",
    "* Epochs > 1024 take a while, but tend to lead to better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/512\n",
      "18000/18000 [==============================] - ETA: 0s - loss: 0.8329WARNING:tensorflow:From e:\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "18000/18000 [==============================] - 38s 2ms/sample - loss: 0.8329 - val_loss: 0.7909\n",
      "Epoch 2/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.7852 - val_loss: 0.7539\n",
      "Epoch 3/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.7497 - val_loss: 0.7276\n",
      "Epoch 4/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.7242 - val_loss: 0.7098\n",
      "Epoch 5/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.7067 - val_loss: 0.6986\n",
      "Epoch 6/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6953 - val_loss: 0.6923\n",
      "Epoch 7/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6885 - val_loss: 0.6894\n",
      "Epoch 8/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6849 - val_loss: 0.6887\n",
      "Epoch 9/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6833 - val_loss: 0.6889\n",
      "Epoch 10/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6828 - val_loss: 0.6891\n",
      "Epoch 11/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6825 - val_loss: 0.6891\n",
      "Epoch 12/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6823 - val_loss: 0.6888\n",
      "Epoch 13/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6820 - val_loss: 0.6886\n",
      "Epoch 14/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6817 - val_loss: 0.6882\n",
      "Epoch 15/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6814 - val_loss: 0.6878\n",
      "Epoch 16/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6811 - val_loss: 0.6877\n",
      "Epoch 17/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6808 - val_loss: 0.6874\n",
      "Epoch 18/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6805 - val_loss: 0.6870\n",
      "Epoch 19/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6803 - val_loss: 0.6868\n",
      "Epoch 20/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6800 - val_loss: 0.6866\n",
      "Epoch 21/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6797 - val_loss: 0.6863\n",
      "Epoch 22/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6795 - val_loss: 0.6860\n",
      "Epoch 23/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6792 - val_loss: 0.6858\n",
      "Epoch 24/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6790 - val_loss: 0.6857\n",
      "Epoch 25/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6788 - val_loss: 0.6854\n",
      "Epoch 26/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6785 - val_loss: 0.6851\n",
      "Epoch 27/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6783 - val_loss: 0.6849\n",
      "Epoch 28/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6781 - val_loss: 0.6847\n",
      "Epoch 29/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6779 - val_loss: 0.6846\n",
      "Epoch 30/512\n",
      "18000/18000 [==============================] - 38s 2ms/sample - loss: 0.6777 - val_loss: 0.6843\n",
      "Epoch 31/512\n",
      "18000/18000 [==============================] - 38s 2ms/sample - loss: 0.6775 - val_loss: 0.6841\n",
      "Epoch 32/512\n",
      "18000/18000 [==============================] - 38s 2ms/sample - loss: 0.6773 - val_loss: 0.6840\n",
      "Epoch 33/512\n",
      "18000/18000 [==============================] - 38s 2ms/sample - loss: 0.6771 - val_loss: 0.6838\n",
      "Epoch 34/512\n",
      "18000/18000 [==============================] - 38s 2ms/sample - loss: 0.6769 - val_loss: 0.6835\n",
      "Epoch 35/512\n",
      "18000/18000 [==============================] - 38s 2ms/sample - loss: 0.6767 - val_loss: 0.6834\n",
      "Epoch 36/512\n",
      "18000/18000 [==============================] - 38s 2ms/sample - loss: 0.6765 - val_loss: 0.6832\n",
      "Epoch 37/512\n",
      "18000/18000 [==============================] - 38s 2ms/sample - loss: 0.6764 - val_loss: 0.6830\n",
      "Epoch 38/512\n",
      "18000/18000 [==============================] - 38s 2ms/sample - loss: 0.6762 - val_loss: 0.6828\n",
      "Epoch 39/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6760 - val_loss: 0.6826\n",
      "Epoch 40/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6759 - val_loss: 0.6825\n",
      "Epoch 41/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6757 - val_loss: 0.6824\n",
      "Epoch 42/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6756 - val_loss: 0.6822\n",
      "Epoch 43/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6754 - val_loss: 0.6820\n",
      "Epoch 44/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6753 - val_loss: 0.6820\n",
      "Epoch 45/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6751 - val_loss: 0.6818\n",
      "Epoch 46/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6750 - val_loss: 0.6815\n",
      "Epoch 47/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6748 - val_loss: 0.6814\n",
      "Epoch 48/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6747 - val_loss: 0.6813\n",
      "Epoch 49/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6746 - val_loss: 0.6813\n",
      "Epoch 50/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6744 - val_loss: 0.6810\n",
      "Epoch 51/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6743 - val_loss: 0.6809\n",
      "Epoch 52/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6742 - val_loss: 0.6808\n",
      "Epoch 53/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6741 - val_loss: 0.6807\n",
      "Epoch 54/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6739 - val_loss: 0.6805\n",
      "Epoch 55/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6738 - val_loss: 0.6804\n",
      "Epoch 56/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6737 - val_loss: 0.6804\n",
      "Epoch 57/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6736 - val_loss: 0.6803\n",
      "Epoch 58/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6735 - val_loss: 0.6802\n",
      "Epoch 59/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6734 - val_loss: 0.6800\n",
      "Epoch 60/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6733 - val_loss: 0.6799\n",
      "Epoch 61/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6732 - val_loss: 0.6798\n",
      "Epoch 62/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6731 - val_loss: 0.6796\n",
      "Epoch 63/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6730 - val_loss: 0.6796\n",
      "Epoch 64/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6729 - val_loss: 0.6795\n",
      "Epoch 65/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6728 - val_loss: 0.6795\n",
      "Epoch 66/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6727 - val_loss: 0.6793\n",
      "Epoch 67/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6726 - val_loss: 0.6793\n",
      "Epoch 68/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6725 - val_loss: 0.6792\n",
      "Epoch 69/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6724 - val_loss: 0.6791\n",
      "Epoch 70/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6723 - val_loss: 0.6790\n",
      "Epoch 71/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6723 - val_loss: 0.6789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6722 - val_loss: 0.6788\n",
      "Epoch 73/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6721 - val_loss: 0.6787\n",
      "Epoch 74/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6720 - val_loss: 0.6786\n",
      "Epoch 75/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6719 - val_loss: 0.6787\n",
      "Epoch 76/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6719 - val_loss: 0.6785\n",
      "Epoch 77/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6718 - val_loss: 0.6784\n",
      "Epoch 78/512\n",
      "18000/18000 [==============================] - 37s 2ms/sample - loss: 0.6717 - val_loss: 0.6783\n",
      "Epoch 79/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6716 - val_loss: 0.6782\n",
      "Epoch 80/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6716 - val_loss: 0.6782\n",
      "Epoch 81/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6715 - val_loss: 0.6782\n",
      "Epoch 82/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6714 - val_loss: 0.6780\n",
      "Epoch 83/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6714 - val_loss: 0.6780\n",
      "Epoch 84/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6713 - val_loss: 0.6778\n",
      "Epoch 85/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6712 - val_loss: 0.6778\n",
      "Epoch 86/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6712 - val_loss: 0.6778\n",
      "Epoch 87/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6711 - val_loss: 0.6779\n",
      "Epoch 88/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6710 - val_loss: 0.6778\n",
      "Epoch 89/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6710 - val_loss: 0.6774\n",
      "Epoch 90/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6709 - val_loss: 0.6776\n",
      "Epoch 91/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6709 - val_loss: 0.6775\n",
      "Epoch 92/512\n",
      "18000/18000 [==============================] - 36s 2ms/sample - loss: 0.6708 - val_loss: 0.6774\n",
      "Epoch 93/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6707 - val_loss: 0.6774\n",
      "Epoch 94/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6707 - val_loss: 0.6773\n",
      "Epoch 95/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6706 - val_loss: 0.6772\n",
      "Epoch 96/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6706 - val_loss: 0.6773\n",
      "Epoch 97/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6705 - val_loss: 0.6771\n",
      "Epoch 98/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6705 - val_loss: 0.6770\n",
      "Epoch 99/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6704 - val_loss: 0.6770\n",
      "Epoch 100/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6704 - val_loss: 0.6770\n",
      "Epoch 101/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6703 - val_loss: 0.6770\n",
      "Epoch 102/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6703 - val_loss: 0.6768\n",
      "Epoch 103/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6702 - val_loss: 0.6768\n",
      "Epoch 104/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6702 - val_loss: 0.6768\n",
      "Epoch 105/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6701 - val_loss: 0.6767\n",
      "Epoch 106/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6701 - val_loss: 0.6767\n",
      "Epoch 107/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6700 - val_loss: 0.6766\n",
      "Epoch 108/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6700 - val_loss: 0.6766\n",
      "Epoch 109/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6700 - val_loss: 0.6765\n",
      "Epoch 110/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6699 - val_loss: 0.6764\n",
      "Epoch 111/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6699 - val_loss: 0.6764\n",
      "Epoch 112/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6698 - val_loss: 0.6764\n",
      "Epoch 113/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6698 - val_loss: 0.6764\n",
      "Epoch 114/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6698 - val_loss: 0.6763\n",
      "Epoch 115/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6697 - val_loss: 0.6763\n",
      "Epoch 116/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6697 - val_loss: 0.6762\n",
      "Epoch 117/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6696 - val_loss: 0.6761\n",
      "Epoch 118/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6696 - val_loss: 0.6762\n",
      "Epoch 119/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6696 - val_loss: 0.6761\n",
      "Epoch 120/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6695 - val_loss: 0.6761\n",
      "Epoch 121/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6695 - val_loss: 0.6761\n",
      "Epoch 122/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6695 - val_loss: 0.6760\n",
      "Epoch 123/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6694 - val_loss: 0.6759\n",
      "Epoch 124/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6694 - val_loss: 0.6759\n",
      "Epoch 125/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6693 - val_loss: 0.6759\n",
      "Epoch 126/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6693 - val_loss: 0.6758\n",
      "Epoch 127/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6693 - val_loss: 0.6760\n",
      "Epoch 128/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6692 - val_loss: 0.6758\n",
      "Epoch 129/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6692 - val_loss: 0.6757\n",
      "Epoch 130/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6692 - val_loss: 0.6757\n",
      "Epoch 131/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6692 - val_loss: 0.6757\n",
      "Epoch 132/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6691 - val_loss: 0.6756\n",
      "Epoch 133/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6691 - val_loss: 0.6756\n",
      "Epoch 134/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6691 - val_loss: 0.6755\n",
      "Epoch 135/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6690 - val_loss: 0.6754\n",
      "Epoch 136/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6690 - val_loss: 0.6754\n",
      "Epoch 137/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6690 - val_loss: 0.6754\n",
      "Epoch 138/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6689 - val_loss: 0.6755\n",
      "Epoch 139/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6689 - val_loss: 0.6753\n",
      "Epoch 140/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6689 - val_loss: 0.6754\n",
      "Epoch 141/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6688 - val_loss: 0.6753\n",
      "Epoch 142/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6688 - val_loss: 0.6752\n",
      "Epoch 143/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6688 - val_loss: 0.6753\n",
      "Epoch 144/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6687 - val_loss: 0.6751\n",
      "Epoch 145/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6687 - val_loss: 0.6752\n",
      "Epoch 146/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6687 - val_loss: 0.6752\n",
      "Epoch 147/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6687 - val_loss: 0.6750\n",
      "Epoch 148/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6686 - val_loss: 0.6750\n",
      "Epoch 149/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6686 - val_loss: 0.6750\n",
      "Epoch 150/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6686 - val_loss: 0.6751\n",
      "Epoch 151/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6685 - val_loss: 0.6751\n",
      "Epoch 152/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6685 - val_loss: 0.6749\n",
      "Epoch 153/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6685 - val_loss: 0.6749\n",
      "Epoch 154/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6685 - val_loss: 0.6749\n",
      "Epoch 155/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6684 - val_loss: 0.6748\n",
      "Epoch 156/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6684 - val_loss: 0.6749\n",
      "Epoch 157/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6684 - val_loss: 0.6748\n",
      "Epoch 158/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6684 - val_loss: 0.6748\n",
      "Epoch 159/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6683 - val_loss: 0.6747\n",
      "Epoch 160/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6683 - val_loss: 0.6747\n",
      "Epoch 161/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6683 - val_loss: 0.6747\n",
      "Epoch 162/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6683 - val_loss: 0.6747\n",
      "Epoch 163/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6682 - val_loss: 0.6747\n",
      "Epoch 164/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6682 - val_loss: 0.6746\n",
      "Epoch 165/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6682 - val_loss: 0.6746\n",
      "Epoch 166/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6682 - val_loss: 0.6746\n",
      "Epoch 167/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6681 - val_loss: 0.6747\n",
      "Epoch 168/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6681 - val_loss: 0.6745\n",
      "Epoch 169/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6681 - val_loss: 0.6746\n",
      "Epoch 170/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6681 - val_loss: 0.6744\n",
      "Epoch 171/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6680 - val_loss: 0.6745\n",
      "Epoch 172/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6680 - val_loss: 0.6744\n",
      "Epoch 173/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6680 - val_loss: 0.6743\n",
      "Epoch 174/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6680 - val_loss: 0.6744\n",
      "Epoch 175/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6679 - val_loss: 0.6743\n",
      "Epoch 176/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6679 - val_loss: 0.6744\n",
      "Epoch 177/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6679 - val_loss: 0.6742\n",
      "Epoch 178/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6679 - val_loss: 0.6743\n",
      "Epoch 179/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6679 - val_loss: 0.6741\n",
      "Epoch 180/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6678 - val_loss: 0.6741\n",
      "Epoch 181/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6678 - val_loss: 0.6741\n",
      "Epoch 182/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6678 - val_loss: 0.6742\n",
      "Epoch 183/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6678 - val_loss: 0.6741\n",
      "Epoch 184/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6677 - val_loss: 0.6740\n",
      "Epoch 185/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6677 - val_loss: 0.6741\n",
      "Epoch 186/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6677 - val_loss: 0.6741\n",
      "Epoch 187/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6677 - val_loss: 0.6740\n",
      "Epoch 188/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6677 - val_loss: 0.6740\n",
      "Epoch 189/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6676 - val_loss: 0.6739\n",
      "Epoch 190/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6676 - val_loss: 0.6741\n",
      "Epoch 191/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6676 - val_loss: 0.6739\n",
      "Epoch 192/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6676 - val_loss: 0.6741\n",
      "Epoch 193/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6675 - val_loss: 0.6738\n",
      "Epoch 194/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6675 - val_loss: 0.6739\n",
      "Epoch 195/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6675 - val_loss: 0.6738\n",
      "Epoch 196/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6675 - val_loss: 0.6737\n",
      "Epoch 197/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6674 - val_loss: 0.6738\n",
      "Epoch 198/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6674 - val_loss: 0.6737\n",
      "Epoch 199/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6674 - val_loss: 0.6737\n",
      "Epoch 200/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6674 - val_loss: 0.6736\n",
      "Epoch 201/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6674 - val_loss: 0.6737\n",
      "Epoch 202/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6673 - val_loss: 0.6737\n",
      "Epoch 203/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6673 - val_loss: 0.6736\n",
      "Epoch 204/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6673 - val_loss: 0.6736\n",
      "Epoch 205/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6673 - val_loss: 0.6738\n",
      "Epoch 206/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6673 - val_loss: 0.6737\n",
      "Epoch 207/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6672 - val_loss: 0.6736\n",
      "Epoch 208/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6672 - val_loss: 0.6736\n",
      "Epoch 209/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6672 - val_loss: 0.6735\n",
      "Epoch 210/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6672 - val_loss: 0.6734\n",
      "Epoch 211/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6672 - val_loss: 0.6733\n",
      "Epoch 212/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6671 - val_loss: 0.6734\n",
      "Epoch 213/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6671 - val_loss: 0.6734\n",
      "Epoch 214/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6671 - val_loss: 0.6733\n",
      "Epoch 215/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6671 - val_loss: 0.6734\n",
      "Epoch 216/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6670 - val_loss: 0.6733\n",
      "Epoch 217/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6670 - val_loss: 0.6732\n",
      "Epoch 218/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6670 - val_loss: 0.6733\n",
      "Epoch 219/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6670 - val_loss: 0.6732\n",
      "Epoch 220/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6670 - val_loss: 0.6732\n",
      "Epoch 221/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6669 - val_loss: 0.6732\n",
      "Epoch 222/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6669 - val_loss: 0.6731\n",
      "Epoch 223/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6669 - val_loss: 0.6731\n",
      "Epoch 224/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6669 - val_loss: 0.6732\n",
      "Epoch 225/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6669 - val_loss: 0.6731\n",
      "Epoch 226/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6668 - val_loss: 0.6731\n",
      "Epoch 227/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6668 - val_loss: 0.6732\n",
      "Epoch 228/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6668 - val_loss: 0.6731\n",
      "Epoch 229/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6668 - val_loss: 0.6731\n",
      "Epoch 230/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6668 - val_loss: 0.6730\n",
      "Epoch 231/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6667 - val_loss: 0.6729\n",
      "Epoch 232/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6667 - val_loss: 0.6729\n",
      "Epoch 233/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6667 - val_loss: 0.6729\n",
      "Epoch 234/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6667 - val_loss: 0.6729\n",
      "Epoch 235/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6667 - val_loss: 0.6728\n",
      "Epoch 236/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6666 - val_loss: 0.6728\n",
      "Epoch 237/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6666 - val_loss: 0.6728\n",
      "Epoch 238/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6666 - val_loss: 0.6727\n",
      "Epoch 239/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6666 - val_loss: 0.6727\n",
      "Epoch 240/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6666 - val_loss: 0.6726\n",
      "Epoch 241/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6665 - val_loss: 0.6727\n",
      "Epoch 242/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6665 - val_loss: 0.6728\n",
      "Epoch 243/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6665 - val_loss: 0.6728\n",
      "Epoch 244/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6665 - val_loss: 0.6726\n",
      "Epoch 245/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6665 - val_loss: 0.6726\n",
      "Epoch 246/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6664 - val_loss: 0.6726\n",
      "Epoch 247/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6664 - val_loss: 0.6726\n",
      "Epoch 248/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6664 - val_loss: 0.6727\n",
      "Epoch 249/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6664 - val_loss: 0.6725\n",
      "Epoch 250/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6664 - val_loss: 0.6726\n",
      "Epoch 251/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6663 - val_loss: 0.6724\n",
      "Epoch 252/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6663 - val_loss: 0.6724\n",
      "Epoch 253/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6663 - val_loss: 0.6724\n",
      "Epoch 254/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6663 - val_loss: 0.6724\n",
      "Epoch 255/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6662 - val_loss: 0.6724\n",
      "Epoch 256/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6662 - val_loss: 0.6724\n",
      "Epoch 257/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6662 - val_loss: 0.6724\n",
      "Epoch 258/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6662 - val_loss: 0.6723\n",
      "Epoch 259/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6662 - val_loss: 0.6724\n",
      "Epoch 260/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6661 - val_loss: 0.6723\n",
      "Epoch 261/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6661 - val_loss: 0.6724\n",
      "Epoch 262/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6661 - val_loss: 0.6722\n",
      "Epoch 263/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6661 - val_loss: 0.6722\n",
      "Epoch 264/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6661 - val_loss: 0.6721\n",
      "Epoch 265/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6660 - val_loss: 0.6721\n",
      "Epoch 266/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6660 - val_loss: 0.6721\n",
      "Epoch 267/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6660 - val_loss: 0.6721\n",
      "Epoch 268/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6660 - val_loss: 0.6721\n",
      "Epoch 269/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6660 - val_loss: 0.6720\n",
      "Epoch 270/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6659 - val_loss: 0.6721\n",
      "Epoch 271/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6659 - val_loss: 0.6720\n",
      "Epoch 272/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6659 - val_loss: 0.6721\n",
      "Epoch 273/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6659 - val_loss: 0.6719\n",
      "Epoch 274/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6659 - val_loss: 0.6720\n",
      "Epoch 275/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6658 - val_loss: 0.6718\n",
      "Epoch 276/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6658 - val_loss: 0.6719\n",
      "Epoch 277/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6658 - val_loss: 0.6719\n",
      "Epoch 278/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6658 - val_loss: 0.6720\n",
      "Epoch 279/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6657 - val_loss: 0.6719\n",
      "Epoch 280/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6657 - val_loss: 0.6718\n",
      "Epoch 281/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6657 - val_loss: 0.6718\n",
      "Epoch 282/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6657 - val_loss: 0.6717\n",
      "Epoch 283/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6657 - val_loss: 0.6719\n",
      "Epoch 284/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6656 - val_loss: 0.6718\n",
      "Epoch 285/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6656 - val_loss: 0.6717\n",
      "Epoch 286/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6656 - val_loss: 0.6717\n",
      "Epoch 287/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6656 - val_loss: 0.6716\n",
      "Epoch 288/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6656 - val_loss: 0.6716\n",
      "Epoch 289/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6655 - val_loss: 0.6715\n",
      "Epoch 290/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6655 - val_loss: 0.6717\n",
      "Epoch 291/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6655 - val_loss: 0.6715\n",
      "Epoch 292/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6655 - val_loss: 0.6715\n",
      "Epoch 293/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6655 - val_loss: 0.6716\n",
      "Epoch 294/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6654 - val_loss: 0.6714\n",
      "Epoch 295/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6654 - val_loss: 0.6714\n",
      "Epoch 296/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6654 - val_loss: 0.6716\n",
      "Epoch 297/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6654 - val_loss: 0.6714\n",
      "Epoch 298/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6654 - val_loss: 0.6713\n",
      "Epoch 299/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6653 - val_loss: 0.6714\n",
      "Epoch 300/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6653 - val_loss: 0.6715\n",
      "Epoch 301/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6653 - val_loss: 0.6713\n",
      "Epoch 302/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6653 - val_loss: 0.6714\n",
      "Epoch 303/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6652 - val_loss: 0.6712\n",
      "Epoch 304/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6652 - val_loss: 0.6712\n",
      "Epoch 305/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6652 - val_loss: 0.6711\n",
      "Epoch 306/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6652 - val_loss: 0.6712\n",
      "Epoch 307/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6652 - val_loss: 0.6712\n",
      "Epoch 308/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6651 - val_loss: 0.6712\n",
      "Epoch 309/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6651 - val_loss: 0.6711\n",
      "Epoch 310/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6651 - val_loss: 0.6712\n",
      "Epoch 311/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6651 - val_loss: 0.6711\n",
      "Epoch 312/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6651 - val_loss: 0.6711\n",
      "Epoch 313/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6650 - val_loss: 0.6710\n",
      "Epoch 314/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6650 - val_loss: 0.6711\n",
      "Epoch 315/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6650 - val_loss: 0.6709\n",
      "Epoch 316/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6650 - val_loss: 0.6710\n",
      "Epoch 317/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6649 - val_loss: 0.6708\n",
      "Epoch 318/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6649 - val_loss: 0.6710\n",
      "Epoch 319/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6649 - val_loss: 0.6708\n",
      "Epoch 320/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6649 - val_loss: 0.6708\n",
      "Epoch 321/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6649 - val_loss: 0.6708\n",
      "Epoch 322/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6648 - val_loss: 0.6708\n",
      "Epoch 323/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6648 - val_loss: 0.6708\n",
      "Epoch 324/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6648 - val_loss: 0.6707\n",
      "Epoch 325/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6648 - val_loss: 0.6707\n",
      "Epoch 326/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6647 - val_loss: 0.6708\n",
      "Epoch 327/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6647 - val_loss: 0.6707\n",
      "Epoch 328/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6647 - val_loss: 0.6706\n",
      "Epoch 329/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6647 - val_loss: 0.6706\n",
      "Epoch 330/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6647 - val_loss: 0.6707\n",
      "Epoch 331/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6646 - val_loss: 0.6707\n",
      "Epoch 332/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6646 - val_loss: 0.6706\n",
      "Epoch 333/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6646 - val_loss: 0.6706\n",
      "Epoch 334/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6646 - val_loss: 0.6707\n",
      "Epoch 335/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6645 - val_loss: 0.6705\n",
      "Epoch 336/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6645 - val_loss: 0.6706\n",
      "Epoch 337/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6645 - val_loss: 0.6705\n",
      "Epoch 338/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6645 - val_loss: 0.6704\n",
      "Epoch 339/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6645 - val_loss: 0.6705\n",
      "Epoch 340/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6644 - val_loss: 0.6704\n",
      "Epoch 341/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6644 - val_loss: 0.6703\n",
      "Epoch 342/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6644 - val_loss: 0.6703\n",
      "Epoch 343/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6644 - val_loss: 0.6703\n",
      "Epoch 344/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6643 - val_loss: 0.6703\n",
      "Epoch 345/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6643 - val_loss: 0.6702\n",
      "Epoch 346/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6643 - val_loss: 0.6703\n",
      "Epoch 347/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6643 - val_loss: 0.6703\n",
      "Epoch 348/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6643 - val_loss: 0.6702\n",
      "Epoch 349/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6642 - val_loss: 0.6702\n",
      "Epoch 350/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6642 - val_loss: 0.6700\n",
      "Epoch 351/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6642 - val_loss: 0.6702\n",
      "Epoch 352/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6642 - val_loss: 0.6700\n",
      "Epoch 353/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6641 - val_loss: 0.6700\n",
      "Epoch 354/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6641 - val_loss: 0.6700\n",
      "Epoch 355/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6641 - val_loss: 0.6699\n",
      "Epoch 356/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6641 - val_loss: 0.6700\n",
      "Epoch 357/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6640 - val_loss: 0.6700\n",
      "Epoch 358/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6640 - val_loss: 0.6699\n",
      "Epoch 359/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6640 - val_loss: 0.6698\n",
      "Epoch 360/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6640 - val_loss: 0.6697\n",
      "Epoch 361/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6639 - val_loss: 0.6699\n",
      "Epoch 362/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6639 - val_loss: 0.6697\n",
      "Epoch 363/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6639 - val_loss: 0.6699\n",
      "Epoch 364/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6639 - val_loss: 0.6697\n",
      "Epoch 365/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6639 - val_loss: 0.6697\n",
      "Epoch 366/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6638 - val_loss: 0.6698\n",
      "Epoch 367/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6638 - val_loss: 0.6697\n",
      "Epoch 368/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6638 - val_loss: 0.6697\n",
      "Epoch 369/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6638 - val_loss: 0.6697\n",
      "Epoch 370/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6637 - val_loss: 0.6698\n",
      "Epoch 371/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6637 - val_loss: 0.6696\n",
      "Epoch 372/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6637 - val_loss: 0.6694\n",
      "Epoch 373/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6637 - val_loss: 0.6694\n",
      "Epoch 374/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6636 - val_loss: 0.6695\n",
      "Epoch 375/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6636 - val_loss: 0.6694\n",
      "Epoch 376/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6636 - val_loss: 0.6695\n",
      "Epoch 377/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6636 - val_loss: 0.6694\n",
      "Epoch 378/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6635 - val_loss: 0.6694\n",
      "Epoch 379/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6635 - val_loss: 0.6694\n",
      "Epoch 380/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6635 - val_loss: 0.6692\n",
      "Epoch 381/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6635 - val_loss: 0.6695\n",
      "Epoch 382/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6634 - val_loss: 0.6694\n",
      "Epoch 383/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6634 - val_loss: 0.6693\n",
      "Epoch 384/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6634 - val_loss: 0.6692\n",
      "Epoch 385/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6634 - val_loss: 0.6691\n",
      "Epoch 386/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6634 - val_loss: 0.6691\n",
      "Epoch 387/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6633 - val_loss: 0.6694\n",
      "Epoch 388/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6633 - val_loss: 0.6693\n",
      "Epoch 389/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6633 - val_loss: 0.6691\n",
      "Epoch 390/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6633 - val_loss: 0.6692\n",
      "Epoch 391/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6632 - val_loss: 0.6691\n",
      "Epoch 392/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6632 - val_loss: 0.6688\n",
      "Epoch 393/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6632 - val_loss: 0.6689\n",
      "Epoch 394/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6632 - val_loss: 0.6689\n",
      "Epoch 395/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6631 - val_loss: 0.6689\n",
      "Epoch 396/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6631 - val_loss: 0.6690\n",
      "Epoch 397/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6631 - val_loss: 0.6690\n",
      "Epoch 398/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6631 - val_loss: 0.6688\n",
      "Epoch 399/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6630 - val_loss: 0.6687\n",
      "Epoch 400/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6630 - val_loss: 0.6689\n",
      "Epoch 401/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6630 - val_loss: 0.6688\n",
      "Epoch 402/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6630 - val_loss: 0.6688\n",
      "Epoch 403/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6629 - val_loss: 0.6687\n",
      "Epoch 404/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6629 - val_loss: 0.6687\n",
      "Epoch 405/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6629 - val_loss: 0.6686\n",
      "Epoch 406/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6629 - val_loss: 0.6687\n",
      "Epoch 407/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6628 - val_loss: 0.6687\n",
      "Epoch 408/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6628 - val_loss: 0.6686\n",
      "Epoch 409/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6628 - val_loss: 0.6685\n",
      "Epoch 410/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6628 - val_loss: 0.6685\n",
      "Epoch 411/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6627 - val_loss: 0.6686\n",
      "Epoch 412/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6627 - val_loss: 0.6685\n",
      "Epoch 413/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6627 - val_loss: 0.6684\n",
      "Epoch 414/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6627 - val_loss: 0.6684\n",
      "Epoch 415/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6627 - val_loss: 0.6683\n",
      "Epoch 416/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6626 - val_loss: 0.6683\n",
      "Epoch 417/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6626 - val_loss: 0.6685\n",
      "Epoch 418/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6626 - val_loss: 0.6682\n",
      "Epoch 419/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6626 - val_loss: 0.6682\n",
      "Epoch 420/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6625 - val_loss: 0.6682\n",
      "Epoch 421/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6625 - val_loss: 0.6680\n",
      "Epoch 422/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6625 - val_loss: 0.6682\n",
      "Epoch 423/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6625 - val_loss: 0.6682\n",
      "Epoch 424/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6624 - val_loss: 0.6682\n",
      "Epoch 425/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6624 - val_loss: 0.6681\n",
      "Epoch 426/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6624 - val_loss: 0.6681\n",
      "Epoch 427/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6624 - val_loss: 0.6681\n",
      "Epoch 428/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6623 - val_loss: 0.6680\n",
      "Epoch 429/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6623 - val_loss: 0.6681\n",
      "Epoch 430/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6623 - val_loss: 0.6679\n",
      "Epoch 431/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6623 - val_loss: 0.6679\n",
      "Epoch 432/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6622 - val_loss: 0.6680\n",
      "Epoch 433/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6622 - val_loss: 0.6679\n",
      "Epoch 434/512\n",
      "18000/18000 [==============================] - 34s 2ms/sample - loss: 0.6622 - val_loss: 0.6679\n",
      "Epoch 435/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6622 - val_loss: 0.6679\n",
      "Epoch 436/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6621 - val_loss: 0.6677\n",
      "Epoch 437/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6621 - val_loss: 0.6677\n",
      "Epoch 438/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6621 - val_loss: 0.6678\n",
      "Epoch 439/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6621 - val_loss: 0.6677\n",
      "Epoch 440/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6620 - val_loss: 0.6677\n",
      "Epoch 441/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6620 - val_loss: 0.6677\n",
      "Epoch 442/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6620 - val_loss: 0.6675\n",
      "Epoch 443/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6620 - val_loss: 0.6676\n",
      "Epoch 444/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6619 - val_loss: 0.6676\n",
      "Epoch 445/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6619 - val_loss: 0.6675\n",
      "Epoch 446/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6619 - val_loss: 0.6677\n",
      "Epoch 447/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6619 - val_loss: 0.6675\n",
      "Epoch 448/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6619 - val_loss: 0.6677\n",
      "Epoch 449/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6618 - val_loss: 0.6673\n",
      "Epoch 450/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6618 - val_loss: 0.6673\n",
      "Epoch 451/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6618 - val_loss: 0.6674\n",
      "Epoch 452/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6618 - val_loss: 0.6672\n",
      "Epoch 453/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6617 - val_loss: 0.6673\n",
      "Epoch 454/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6617 - val_loss: 0.6674\n",
      "Epoch 455/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6617 - val_loss: 0.6673\n",
      "Epoch 456/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6617 - val_loss: 0.6674\n",
      "Epoch 457/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6616 - val_loss: 0.6673\n",
      "Epoch 458/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6616 - val_loss: 0.6672\n",
      "Epoch 459/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6616 - val_loss: 0.6674\n",
      "Epoch 460/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6616 - val_loss: 0.6671\n",
      "Epoch 461/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6615 - val_loss: 0.6673\n",
      "Epoch 462/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6615 - val_loss: 0.6672\n",
      "Epoch 463/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6615 - val_loss: 0.6670\n",
      "Epoch 464/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6615 - val_loss: 0.6670\n",
      "Epoch 465/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6614 - val_loss: 0.6670\n",
      "Epoch 466/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6614 - val_loss: 0.6673\n",
      "Epoch 467/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6614 - val_loss: 0.6670\n",
      "Epoch 468/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6614 - val_loss: 0.6670\n",
      "Epoch 469/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6614 - val_loss: 0.6669\n",
      "Epoch 470/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6613 - val_loss: 0.6670\n",
      "Epoch 471/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6613 - val_loss: 0.6669\n",
      "Epoch 472/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6613 - val_loss: 0.6670\n",
      "Epoch 473/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6613 - val_loss: 0.6668\n",
      "Epoch 474/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6612 - val_loss: 0.6667\n",
      "Epoch 475/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6612 - val_loss: 0.6668\n",
      "Epoch 476/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6612 - val_loss: 0.6666\n",
      "Epoch 477/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6612 - val_loss: 0.6668\n",
      "Epoch 478/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6611 - val_loss: 0.6667\n",
      "Epoch 479/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6611 - val_loss: 0.6665\n",
      "Epoch 480/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6611 - val_loss: 0.6666\n",
      "Epoch 481/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6611 - val_loss: 0.6666\n",
      "Epoch 482/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6610 - val_loss: 0.6666\n",
      "Epoch 483/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6610 - val_loss: 0.6666\n",
      "Epoch 484/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6610 - val_loss: 0.6667\n",
      "Epoch 485/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6610 - val_loss: 0.6666\n",
      "Epoch 486/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6610 - val_loss: 0.6665\n",
      "Epoch 487/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6609 - val_loss: 0.6665\n",
      "Epoch 488/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6609 - val_loss: 0.6664\n",
      "Epoch 489/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6609 - val_loss: 0.6667\n",
      "Epoch 490/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6609 - val_loss: 0.6665\n",
      "Epoch 491/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6608 - val_loss: 0.6665\n",
      "Epoch 492/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6608 - val_loss: 0.6665\n",
      "Epoch 493/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6608 - val_loss: 0.6663\n",
      "Epoch 494/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6608 - val_loss: 0.6663\n",
      "Epoch 495/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6608 - val_loss: 0.6662\n",
      "Epoch 496/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6607 - val_loss: 0.6662\n",
      "Epoch 497/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6607 - val_loss: 0.6663\n",
      "Epoch 498/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6607 - val_loss: 0.6662\n",
      "Epoch 499/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6607 - val_loss: 0.6662\n",
      "Epoch 500/512\n",
      "18000/18000 [==============================] - 34s 2ms/sample - loss: 0.6607 - val_loss: 0.6663\n",
      "Epoch 501/512\n",
      "18000/18000 [==============================] - 34s 2ms/sample - loss: 0.6606 - val_loss: 0.6662\n",
      "Epoch 502/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6606 - val_loss: 0.6661\n",
      "Epoch 503/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6606 - val_loss: 0.6661\n",
      "Epoch 504/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6606 - val_loss: 0.6661\n",
      "Epoch 505/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6605 - val_loss: 0.6660\n",
      "Epoch 506/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6605 - val_loss: 0.6662\n",
      "Epoch 507/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6605 - val_loss: 0.6662\n",
      "Epoch 508/512\n",
      "18000/18000 [==============================] - 34s 2ms/sample - loss: 0.6605 - val_loss: 0.6661\n",
      "Epoch 509/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6605 - val_loss: 0.6660\n",
      "Epoch 510/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6604 - val_loss: 0.6659\n",
      "Epoch 511/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6604 - val_loss: 0.6658\n",
      "Epoch 512/512\n",
      "18000/18000 [==============================] - 35s 2ms/sample - loss: 0.6604 - val_loss: 0.6660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ae77d11760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train, \n",
    "                epochs=512, batch_size=256, \n",
    "                shuffle=True, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Reconstructing Images\n",
    "To qualitatively assess the model, look at a few images and their reconstructions from latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "\n",
    "# @lru_cache(maxsize=None)\n",
    "def decode_latent(decoder, latent_vector:tuple):\n",
    "    \"\"\"\n",
    "    helper to decode latent, with caching to speed up\n",
    "    # Arguments\n",
    "        decode_only: model for decoding\n",
    "        latent_vector: _tuple_ representing the vector to be decoded\n",
    "    # Returns\n",
    "        decoded image\n",
    "    \"\"\"\n",
    "    latent_vector_arr = np.array([latent_vector])\n",
    "    return decoder.predict(latent_vector_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cxr_projection.show_original_decoded import show_grayscale, show_original_decoded\n",
    "\n",
    "encode_only_imgs = encoder.predict(x_test[:10])\n",
    "decoded_imgs = \\\n",
    "    [decode_latent(decoder, tuple(latent_vector)) \n",
    "         for latent_vector in encode_only_imgs[2]]\n",
    "# show_original_decoded(x_test[:10], decoded_imgs, sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 3 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a4a99541e1b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mlevel_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoded_imgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'plasma'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# , vmin=-.5, vmax=.5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 3 with size 1"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(2,2)\n",
    "for n in range(4):\n",
    "    level_img = np.reshape(decoded_imgs[6][...,n], (128,128))\n",
    "    axs[n // 2][n % 2].imshow(level_img, cmap='plasma') # , vmin=-.5, vmax=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.filter_banks import make_gauss_kernels, make_dog_kernels, show_filter_bank\n",
    "dog_kernels_tf = make_dog_kernels()\n",
    "show_filter_bank(dog_kernels_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_all = [-0.3100704,  -0.23559849, -0.02646105,  0.        ] \n",
    "# max_all = [0.42579,    0.38077682, 0.44011822, 0.54857194]\n",
    "#width_all = [0.7358604,  0.6163753,  0.46657926, 0.54857194]\n",
    "\n",
    "re_decoded_img = decoded_imgs[6]\n",
    "re_decoded_img = width_all * re_decoded_img + min_all\n",
    "# print(decoded_imgs[6])\n",
    "print(re_decoded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import tensorflow as tf\n",
    "def mse_tf(current_tf:tf.Tensor, kernel_tf:tf.Tensor, match_tf:tf.Tensor):\n",
    "    current_out_tf = \\\n",
    "        tf.nn.conv2d(current_tf, kernel_tf,\n",
    "                     strides=[1, 1, 1, 1],\n",
    "                     padding='SAME')\n",
    "    loss = tf.reduce_mean((current_out_tf - match_tf) ** 2)\n",
    "    return loss\n",
    "\n",
    "sz = 128\n",
    "current_value = tf.Variable(np.zeros((1,sz,sz,1)), dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    current_out = mse_tf(current_value, dog_kernels_tf, re_decoded_img)\n",
    "grad = tape.gradient(current_out, current_value)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(5,3))\n",
    "axs[0].imshow(tf.reshape(current_value, (sz,sz)))\n",
    "axs[1].imshow(tf.reshape(grad, (sz,sz)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig, axs = plt.subplots(1,2,figsize=(5,3))\n",
    "learning_rate = 1e+3\n",
    "for n in range(100):\n",
    "    current_value.assign_sub(learning_rate * grad)\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_out = mse_tf(current_value, dog_kernels_tf, re_decoded_img)\n",
    "        print(f\"loss = {current_out}   \", end='\\r')\n",
    "    grad = tape.gradient(current_out, current_value)\n",
    "axs[0].imshow(tf.reshape(current_value, (sz,sz)), cmap='gray')\n",
    "axs[1].imshow(tf.reshape(grad, (sz,sz)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK? so then save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from common.image_preprocessing import data_all, data_temp\n",
    "\n",
    "weights_cxr8_temp = temp_from_original(cxr8_original_path, PurePath(f\"{sz}x{sz}\") / 'weights')\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "print(\"Saving \", weights_cxr8_temp / f\"{timestamp}_rgc_encoder.h5\")\n",
    "encoder.save_weights(weights_cxr8_temp / f\"{timestamp}_rgc_encoder.h5\")\n",
    "print(\"Saving \", weights_cxr8_temp / f\"{timestamp}_rgc_decoder.h5\")\n",
    "decoder.save_weights(weights_cxr8_temp / f\"{timestamp}_rgc_decoder.h5\")\n",
    "print(\"Saving \", weights_cxr8_temp / f\"{timestamp}_rgc_autoencoder.h5\")\n",
    "autoencoder.save_weights(weights_cxr8_temp / f\"{timestamp}_rgc_autoencoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_processed_path = \"E:\\\\Data\\\\anat0mixer_temp\\\\shifter_processed\"\n",
    "for n in range(len(x_train_shifted)):\n",
    "    np.save(shift_processed_path+f\"\\\\x_train_shifted_{n}\", \n",
    "            x_train_shifted[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(x_test_shifted)):    \n",
    "    np.save(shift_processed_path+f\"\\\\x_test_shifted_{n}\", \n",
    "            x_test_shifted[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "x_train_fns = \\\n",
    "    Path('E:\\\\Data\\\\anat0mixer_temp\\\\shifter_processed') \\\n",
    "        .glob('x_train_shifted_*.npy')\n",
    "x_train_shifted = []\n",
    "for fn in x_train_fns:\n",
    "    x_train_shifted.append(np.load(fn))\n",
    "\n",
    "x_test_fns = \\\n",
    "    Path('E:\\\\Data\\\\anat0mixer_temp\\\\shifter_processed') \\\n",
    "        .glob('x_test_shifted_*.npy')\n",
    "x_test_shifted = []\n",
    "for fn in x_test_fns:\n",
    "    x_test_shifted.append(np.load(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_shifted = np.array(x_train_shifted)\n",
    "x_test_shifted = np.array(x_test_shifted)\n",
    "x_train_shifted.shape, x_test_shifted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train_shifted, x_train_shifted, \n",
    "                epochs=256, batch_size=128, \n",
    "                shuffle=True, validation_data=(x_test_shifted, x_test_shifted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "autoencoder.save_weights('E:\\\\Data\\\\anat0mixer_temp\\\\autoencoder_v1')\n",
    "encoder.save_weights('E:\\\\Data\\\\anat0mixer_temp\\\\encoder_v1')\n",
    "decoder.save_weights('E:\\\\Data\\\\anat0mixer_temp\\\\decoder_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_shift = encoder.predict(x_test_shifted[80:90])\n",
    "decoded_imgs_shift = \\\n",
    "    [decode_latent(decoder, tuple(latent_vector)) \n",
    "         for latent_vector in encoded_shift[2]]\n",
    "show_original_decoded(x_test_shifted[80:90], decoded_imgs_shift, sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "If the model output looks good, then save it for subsequent use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('data\\zebrastack_v0_covidnet_encoder_model')\n",
    "decoder.save('data\\zebrastack_v0_covidnet_decoder_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anat0Mixer Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image_for_sliders(**kwargs):\n",
    "    from scipy.ndimage import zoom\n",
    "    # print(tuple(kwargs.values()))\n",
    "    decoded_pixel_array = decode_latent(decoder, tuple(kwargs.values()))\n",
    "    interp_array = decoded_pixel_array.reshape(sz,sz)\n",
    "    # interp_array = zoom(decoded_pixel_array.reshape(sz,sz), 4.0, order=0)\n",
    "    # interp_array = interp_array.reshape(sz*4, sz*4)\n",
    "    # ax.imshow(interp_array, cmap='gray')\n",
    "    # a=im.get_array()    \n",
    "    im.set_array(interp_array)\n",
    "    fig.canvas.draw()\n",
    "    return kwargs['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from ipywidgets import FloatSlider, interactive_output, Layout, HBox, VBox\n",
    "\n",
    "kwargs = {}\n",
    "\n",
    "for k in range(latent_dim):\n",
    "    kwargs[str(k)] = \\\n",
    "        FloatSlider(value = 0,\n",
    "                    min = -2.0, max = 2.0, \n",
    "                    step = 0.01,\n",
    "                    orientation='vertical',\n",
    "                    layout=Layout(padding='0%'))\n",
    "\n",
    "decoded_pixel_array = decode_latent(decoder, tuple([slider.value for slider in kwargs.values()]))\n",
    "interp_array = decoded_pixel_array.reshape(sz,sz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,3))\n",
    "ax = fig.add_subplot()\n",
    "im = ax.imshow(interp_array, cmap='gray')\n",
    "\n",
    "slider_widgets = interactive_output(update_image_for_sliders, kwargs)\n",
    "slider_widgets.layout.height = '350px'\n",
    "\n",
    "import gc\n",
    "gc.disable()\n",
    "display(HBox(list(kwargs.values()), layout = Layout(padding='0%')))\n",
    "display(slider_widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
