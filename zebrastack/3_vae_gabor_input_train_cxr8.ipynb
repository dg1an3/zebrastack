{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0683780d03ee1195a9e1ac19e401b8f6c3447ee82d0b15d335dacfb764b91f68"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, SpatialDropout2D\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Lambda\n",
    "from tensorflow.keras.layers import LocallyConnected2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.layers import ActivityRegularization\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"\n",
    "    Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    instead of sampling from Q(z|X), sample eps = N(0,I) \n",
    "        then z = z_mean + sqrt(var)*eps    \n",
    "    # Arguments\n",
    "        args (tensor tuple): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"    \n",
    "    z_mean, z_log_var = args\n",
    "    \n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def sampling_mean_log_var(z_mean_log_var):\n",
    "    \"\"\" \"\"\"\n",
    "    batch, dim = K.shape(z_mean_log_var)[0], K.int_shape(z_mean_log_var)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean_log_var[...,0] + K.exp(0.5 * z_mean_log_var[...,1]) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(z_mean, z_log_var, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute VAE loss, using either mse or crossentropy.\n",
    "    # Arguments\n",
    "        z_mean: mean of Q(z|X)\n",
    "        z_log_var: log variance of Q(z|X)\n",
    "        y_true, y_pred: truth and predicated values\n",
    "    # Returns\n",
    "        loss value\n",
    "    \"\"\"\n",
    "    match_loss = mse(K.flatten(y_true), K.flatten(y_pred))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return match_loss + (1e-4 * kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_func = 'softplus' # or 'relu'\n",
    "locally_connected_channels = 2\n",
    "latent_dim = 8\n",
    "sz = 128\n",
    "\n",
    "# this is the V1 filter bank output\n",
    "v1_filtered_output = Input(shape=(sz//2,sz//2,4), name='v1_{}'.format(sz//2))\n",
    "\n",
    "encoder = \\\n",
    "    Sequential(name='v1_to_pulvinar_encoder', layers=\n",
    "    [\n",
    "        v1_filtered_output,\n",
    "        Conv2D(64, (1,1), name='v2_conv2d', activation=act_func, padding='same'),\n",
    "        MaxPooling2D((2,2), name='v2_maxpool', padding='same'),\n",
    "        Conv2D(64, (3,3), name='v4_conv2d', activation=act_func, padding='same'),\n",
    "        MaxPooling2D((2,2), name='v4_maxpool', padding='same'),\n",
    "        Conv2D(64, (1,1), name='pit_conv2d', activation=act_func, padding='same'),\n",
    "        MaxPooling2D((2,2), name='pit_maxpool', padding='same'),\n",
    "        Conv2D(64, (3,3), name='cit_conv2d', activation=act_func, padding='same'),\n",
    "        LocallyConnected2D(locally_connected_channels, (3,3), \n",
    "                           name='ait_local', activation=act_func,\n",
    "                           kernel_regularizer=L1L2(l1=0.01, l2=0.01)),\n",
    "        Flatten(name='pulvinar_flatten'),\n",
    "        Dense((latent_dim*2), activation=act_func, name='pulvinar_dense'),\n",
    "        Reshape((latent_dim,2), name='pulinvar_reshape'),\n",
    "        Lambda(sampling_mean_log_var, output_shape=(latent_dim,1), name='z')\n",
    "    ])\n",
    "encoder_output = encoder(v1_filtered_output)\n",
    "print(encoder_output)\n",
    "encoder.summary()\n",
    "\n",
    "local_shape = encoder.get_layer('ait_local').output_shape\n",
    "z_shape = encoder.get_layer('z').output_shape\n",
    "decoder = \\\n",
    "    Sequential(name='pulvinar_to_v1_decoder', layers=\n",
    "    [\n",
    "        Input(shape=z_shape, name='z_sampling'),\n",
    "        Dense(np.prod(local_shape[1:]), name='pulvinar_dense_back', activation=act_func),\n",
    "        Reshape(local_shape[1:], name='pulvinar_antiflatten'),\n",
    "        ZeroPadding2D(padding=(1,1), name='ait_padding_back'),\n",
    "        LocallyConnected2D(locally_connected_channels, (3,3), \n",
    "                           name='ait_local_back', activation=act_func,\n",
    "                           kernel_regularizer=L1L2(l1=0.01, l2=0.01)),\n",
    "        ZeroPadding2D(padding=(1,1), name='cit_padding_back'),\n",
    "        Conv2DTranspose(64, (3,3), name='cit_conv2d_trans', activation=act_func, padding='same'),       \n",
    "        UpSampling2D((2,2), name='cit_upsample_back'),\n",
    "        Conv2DTranspose(64, (1,1), name='pit_conv2d_trans', activation=act_func, padding='same'),\n",
    "        UpSampling2D((2,2), name='pit_upsample_back'),\n",
    "        Conv2DTranspose(64, (3,3), name='v4_conv2d_trans', activation=act_func, padding='same'),\n",
    "        UpSampling2D((2,2), name='v4_upsample_back'),\n",
    "        Conv2DTranspose(4, (1,1), name='v2_conv2d_trans', activation=act_func, padding='same')\n",
    "    ])\n",
    "decoder_output = decoder(encoder_output)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "autoencoder = Model(v1_filtered_output, decoder_output, name='v1_to_pulvinar_vae')\n",
    "autoencoder.summary()\n",
    "pprint.pprint(autoencoder.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# set up paths\n",
    "nih_cxr8_csv = (Path(os.environ['DATA_NIH_CXR8']) / 'Data_Entry_2017_v2020').with_suffix('.csv')\n",
    "logging.info(f\"Reading from {nih_cxr8_csv}\")\n",
    "\n",
    "# read the csv dataset\n",
    "cxr_batched_ds = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern=nih_cxr8_csv, \n",
    "    batch_size=batch_size, num_epochs=1, num_parallel_reads=20,\n",
    "    shuffle=False)\n",
    "\n",
    "# look at first 50\n",
    "cxr_batched_ds = cxr_batched_ds.take(50)\n",
    "\n",
    "# filter for No Finding\n",
    "nofinding_cxrs = \\\n",
    "    cxr_batched_ds.filter(\n",
    "        lambda item: tf.equal(item['Finding Labels'][0], 'No Finding'))\n",
    "\n",
    "# show a few\n",
    "for batch in nofinding_cxrs.take(3):\n",
    "    logging.info(\"-----------\")\n",
    "    for item in batch:\n",
    "        logging.info(f\"{item}: {batch[item]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform single epoch\n",
    "for batch_x_in in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        batch_x_pred = autoencoder(batch_x_in)\n",
    "        loss = vae_loss(z_mean, z_log_var, batch_x_in, batch_x_pred)\n",
    "    grads = tape.gradient(loss, autoencoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, autoencoder.trainable_variables))\n",
    "    print(grads)"
   ]
  }
 ]
}